{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeddecfc",
   "metadata": {},
   "source": [
    "## NLP-HW1\n",
    "### Student name: Ziyan Jiang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca5fbe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcec5772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file and create a word dictionary \n",
    "def wordsDictionary(inputfile):\n",
    "    wordsDict = dict()\n",
    "    file = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    for line in file:\n",
    "        line = line.lower()\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word in wordsDict:\n",
    "                wordsDict[word] +=1\n",
    "            else:\n",
    "                wordsDict[word]=1\n",
    "    return wordsDict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6375a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the <s> and </s> into the file, and output a new file.\n",
    "def paddingFile(inputfile,outputfile):\n",
    "    inputFile = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    outputFile = open(outputfile,\"w\",encoding=\"utf-8\")\n",
    "    for line in inputFile:\n",
    "        line = line.lower()\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        line = (f\"<s> {line} </s>\\n\")\n",
    "        outputFile.write(line)\n",
    "    outputFile.close\n",
    "    return outputfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1936f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the train data. \n",
    "# Replace all words occurring in the training data once with the token <unk>\n",
    "def modifyTrainData(inputfile,outputfile,rangefile):\n",
    "    inputFile = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    outputFile = open(outputfile,\"w\",encoding=\"utf-8\")\n",
    "    wordsDict = wordsDictionary(rangefile)\n",
    "    for line in inputFile:\n",
    "        for word in line.split():\n",
    "            if wordsDict[word]== 1:\n",
    "                outputFile.write(\" <unk>\")\n",
    "                wordsDict.pop(word)\n",
    "            else:\n",
    "                outputFile.write(\" \"+word)\n",
    "        outputFile.write(\"\\n\")\n",
    "    outputFile.close()\n",
    "    return outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2bd1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the test data.\n",
    "# Everyword in the test data not seen in training should be treated as <unk>.\n",
    "def modifyTestData(inputfile,outputfile,rangefile):\n",
    "    inputFile = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    outputFile = open(outputfile,\"w\",encoding=\"utf-8\")\n",
    "    wordsDict = wordsDictionary(rangefile)\n",
    "    for line in inputFile:\n",
    "        for word in line.split():\n",
    "            if word not in wordsDict:\n",
    "                outputFile.write(\" <unk>\")\n",
    "            else:\n",
    "                outputFile.write(\" \"+word)\n",
    "        outputFile.write(\"\\n\")\n",
    "    outputFile.close()\n",
    "    return outputfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26eec956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# padding train file is \"pad_train.txt\"\n",
    "# pre-processed train file is \"pre_processed_train.txt\"\n",
    "paddingTrain = paddingFile(\"train-Spring2022.txt\",\"pad_train.txt\")\n",
    "processed_Train = modifyTrainData(paddingTrain,\"pre_processed_train.txt\",paddingTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0360b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding test file is \"pad_test.txt\"\n",
    "# pre-processed test file is \"pre_processed_test.txt\"\n",
    "paddingTest = paddingFile(\"test.txt\",\"pad_test.txt\")\n",
    "processed_Test= modifyTestData(paddingTest,\"pre_processed_test.txt\",processed_Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34202c6e",
   "metadata": {},
   "source": [
    "## Answer 1.3 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572b4ff3",
   "metadata": {},
   "source": [
    "### Question 1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb245db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question1: How many word types (unique words) are there in the training corpus?\n",
      "Answer: 39502\n"
     ]
    }
   ],
   "source": [
    "print('Question1: How many word types (unique words) are there in the training corpus?')\n",
    "wordsDict_Q1 = wordsDictionary(processed_Train)\n",
    "del wordsDict_Q1['<s>']\n",
    "trainWordsTypes = len(wordsDict_Q1.keys())\n",
    "print(\"Answer:\",trainWordsTypes )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a16aa1",
   "metadata": {},
   "source": [
    "### Question 1.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae0aa62e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question2: How many word tokens are there in the training corpus?\n",
      "Answer: 2221290\n"
     ]
    }
   ],
   "source": [
    "print(\"Question2: How many word tokens are there in the training corpus?\")\n",
    "wordsDict_Q2 = wordsDictionary(processed_Train)\n",
    "del wordsDict_Q2['<s>']\n",
    "trainTotalTokens = sum(wordsDict_Q2.values())\n",
    "print(\"Answer:\",trainTotalTokens )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35d2cbd",
   "metadata": {},
   "source": [
    "### Question 1.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649c6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the percentage of word tokens and word types in the test corpus did not occur in training \n",
    "def percentageOfMissing(inputfile):\n",
    "    paddingDictOfTest = wordsDictionary(\"pad_test.txt\")\n",
    "    paddingDictOfTrain = wordsDictionary(\"pad_train.txt\")\n",
    "    paddingDictOfTest.pop('<s>')\n",
    "    testTotalTokens = sum(paddingDictOfTest.values())\n",
    "    testTotalTypes = len(paddingDictOfTest.keys())\n",
    "#     print(testTotalTokens)\n",
    "#     print(testTotalTypes)\n",
    "    result =[]\n",
    "    inputFile = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    missDict = dict()\n",
    "    count =0\n",
    "    for line in inputFile:\n",
    "        for word in line.split():\n",
    "            if word not in paddingDictOfTrain:\n",
    "                if word in missDict:\n",
    "                    missDict[word]+=1\n",
    "                else:\n",
    "                    missDict[word]=1\n",
    "                count +=1\n",
    "    percentOfTypes = len(missDict.keys())/testTotalTypes *100;\n",
    "    percentOfTokens = sum(missDict.values())/testTotalTokens *100;\n",
    "    result = [percentOfTypes,percentOfTokens]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3fde26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questin3:\n",
      "The percentage of word tokens in the test corpus did not occur in training:  1.806 %\n",
      "The percentage of word types in the test corpus did not occur in training:  3.926 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Questin3:\")\n",
    "print(\"The percentage of word tokens in the test corpus did not occur in training: \",\n",
    "      \"%.3f\"%percentageOfMissing(\"pad_test.txt\")[1],\"%\")\n",
    "print(\"The percentage of word types in the test corpus did not occur in training: \",\n",
    "      \"%.3f\"%percentageOfMissing(\"pad_test.txt\")[0],\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea9e43a",
   "metadata": {},
   "source": [
    "### Question 1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a312dd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file and create a Bigram types dictionary\n",
    "def BigramDictionary(inputfile):\n",
    "    wordsDict = dict()\n",
    "    file = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    for line in file:\n",
    "        line = line.lower()\n",
    "        words = line.split()\n",
    "        for i in range(len(words)-1):\n",
    "            wordPair = (words[i],words[i+1])\n",
    "            if wordPair in wordsDict:\n",
    "                wordsDict[wordPair] +=1\n",
    "            else:\n",
    "                wordsDict[wordPair]=1\n",
    "    return wordsDict\n",
    "\n",
    "# Read file and create a Bigram types dictionary without <s>\n",
    "def BigramDictionaryWithoutS(inputfile):\n",
    "    wordsDict = dict()\n",
    "    file = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    \n",
    "    for line in file:\n",
    "        line = line.lower()\n",
    "        words = line.split()\n",
    "        for i in range(len(words)-1):\n",
    "            if (words[i] != '<s>'):\n",
    "                wordPair = (words[i],words[i+1])\n",
    "                if wordPair in wordsDict:\n",
    "                    wordsDict[wordPair] +=1\n",
    "                else:\n",
    "                    wordsDict[wordPair]=1\n",
    "    return wordsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bcf0fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the percentage of Bigram types word tokens and word types in the test corpus did not occur in training \n",
    "# the corpus without the <s>\n",
    "def percentageOfMissingBigram(inputfile):\n",
    "    bigramOfTest = BigramDictionaryWithoutS(\"pre_processed_test.txt\")\n",
    "    bigramOfTrain = BigramDictionaryWithoutS(\"pre_processed_train.txt\")\n",
    "#     print(len(bigramOfTest.keys()))\n",
    "#     print(sum(bigramOfTest.values()))\n",
    "    testTotalTypesBigram = len(bigramOfTest.keys())\n",
    "    testTotalTokensBigram = sum(bigramOfTest.values())\n",
    "#     print(testTotalTypesBigram)\n",
    "#     print(testTotalTokensBigram)\n",
    "    result =[]\n",
    "    inputFile = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    missDictBigram = dict()\n",
    "    count =0\n",
    "    for line in inputFile:\n",
    "        allWords = line.split()\n",
    "        for i in range(len(allWords)-1):\n",
    "            if (allWords[i] != '<s>'):\n",
    "                wordPair = (allWords[i],allWords[i+1])\n",
    "                if wordPair not in bigramOfTrain:\n",
    "                    if wordPair in missDictBigram:\n",
    "                        missDictBigram[wordPair]+=1\n",
    "                    else:\n",
    "                        missDictBigram[wordPair]=1\n",
    "                    count +=1\n",
    "#     print(count)\n",
    "#     print(len(missDictBigram.keys()))\n",
    "#     print(sum(missDictBigram.values()))\n",
    "    percentOfTypesBigram = len(missDictBigram.keys())/testTotalTypesBigram *100;\n",
    "    percentOfTokensBigram =count/testTotalTokensBigram *100;\n",
    "    result = [percentOfTypesBigram,percentOfTokensBigram]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16468f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question4:\n",
      "The percentage of bigram types in the test corpus did not occur in training:  26.678 %\n",
      "The percentage of bigram tokens in the test corpus did not occur in training:  23.005 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Question4:\")\n",
    "print(\"The percentage of bigram types in the test corpus did not occur in training: \", \n",
    "      \"%.3f\"%percentageOfMissingBigram(processed_Test)[0],\"%\")\n",
    "print(\"The percentage of bigram tokens in the test corpus did not occur in training: \", \n",
    "      \"%.3f\"%percentageOfMissingBigram(processed_Test)[1],\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110c0ec1",
   "metadata": {},
   "source": [
    "### Question 1.3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26dd574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the words list of input sentence \n",
    "def uniReplaceUNK(sen):\n",
    "    sentence = '<s> '+ sen.lower()+' </s>'\n",
    "    unigramDict_Train = wordsDictionary(processed_Train)\n",
    "    wordsOfSentence = sentence.split()\n",
    "    # replace the words not observed in training corpus to the <unk>\n",
    "    for word in wordsOfSentence:\n",
    "        if word not in unigramDict_Train:\n",
    "            wordsOfSentence = [w.replace(word,'<unk>') for w in wordsOfSentence]\n",
    "    return wordsOfSentence\n",
    "\n",
    "# Calculate the Log probability of Unigram model\n",
    "def unigramLogProb(sentencelist):\n",
    "    sentencelist.remove('<s>')\n",
    "    unigramDict_Train = wordsDictionary(processed_Train)\n",
    "    unigramDict_Train.pop('<s>')\n",
    "    totalTokens = sum(unigramDict_Train.values())\n",
    "    sumOfProb =0.0    \n",
    "    for word in sentencelist:\n",
    "        eachOfProb = math.log(unigramDict_Train[word]/totalTokens,2.0)\n",
    "        sumOfProb += eachOfProb\n",
    "        print( 'Log probability of {0:10} is {1}'.format(word,eachOfProb))\n",
    "    print(\" \")\n",
    "    return sumOfProb   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b4d1b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bigram types word list of input sentence\n",
    "def bigramSentencePair(sentence):\n",
    "    sentence = '<s> '+ sentence.lower()+' </s>'\n",
    "    sentencelist = sentence.split()\n",
    "    bigramSentenceList = list(zip(sentencelist,sentencelist[1:]))\n",
    "    return bigramSentenceList\n",
    "\n",
    "# Calculate the Log probability of Bigram model\n",
    "def bigramLogProb(bigramSentenceList):\n",
    "    bigramDict_Train = BigramDictionary(processed_Train)\n",
    "    unigramDict_Train = wordsDictionary(processed_Train)\n",
    "    resultProb =dict()\n",
    "    bigramProb =0.0\n",
    "    bigramLogProb =0.0\n",
    "    for word in bigramSentenceList:\n",
    "        if word not in bigramDict_Train:\n",
    "            bigramProb = 0\n",
    "        else:\n",
    "            bigramProb = bigramDict_Train[word]/unigramDict_Train[word[0]] \n",
    "        resultProb[word]=bigramProb\n",
    "    #print(resultProb) \n",
    "    for word in resultProb:\n",
    "        if  resultProb.get(word) == 0:\n",
    "            bigramLogProb = 'undefined'\n",
    "            eachOfBigramLogProb = bigramLogProb\n",
    "            print('Log probability of {0:20} is {1}'.format(str(word),eachOfBigramLogProb))\n",
    "            print(\" \")\n",
    "            return bigramLogProb\n",
    "        else:\n",
    "            eachOfBigramLogProb = math.log(resultProb.get(word),2)\n",
    "            bigramLogProb += eachOfBigramLogProb\n",
    "        print('Log probability of {0:20} is {1}'.format(str(word),eachOfBigramLogProb))\n",
    "    print(\" \")\n",
    "    return bigramLogProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "783be6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Log probability of Bigram-Add-One model\n",
    "def bigramAddOneLogProb(bigramSentenceList):   \n",
    "    bigramDict_Train = BigramDictionary(processed_Train)\n",
    "    unigramDict_Train = wordsDictionary(processed_Train)\n",
    "    \n",
    "    resultProb =dict()\n",
    "    bigramProbAdd =0.0\n",
    "    bigramAddOneLogProb =0.0\n",
    "    \n",
    "    for word in bigramSentenceList:\n",
    "        if word not in bigramDict_Train:\n",
    "            bigramProbAdd = 1/(unigramDict_Train[word[0]]+ len(unigramDict_Train))\n",
    "        else:\n",
    "            bigramProbAdd = (bigramDict_Train[word] + 1)/(unigramDict_Train[word[0]]+ len(unigramDict_Train))\n",
    "        resultProb[word]= bigramProbAdd\n",
    "   # print(resultProb)    \n",
    "    for word in resultProb:\n",
    "        if  resultProb.get(word) == 0:\n",
    "            bigramLogProb = 'undefined'\n",
    "            eachOfBigramLogProb = bigramLogProb\n",
    "            print('Log probability of {0:20} is {1}'.format(str(word),eachOfBigramLogProb))\n",
    "            print(\" \")\n",
    "            return bigramLogProb\n",
    "        else:\n",
    "            eachOfBigramLogProb = math.log(resultProb.get(word),2)\n",
    "            bigramAddOneLogProb += eachOfBigramLogProb\n",
    "        print('Log probability of {0:20} is {1}'.format(str(word),eachOfBigramLogProb))\n",
    "    print(\" \")\n",
    "    return bigramAddOneLogProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f052e31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5:\n",
      "The sentence:  <s> I look forward to hearing your reply . </s>\n",
      "Unigram Model:\n",
      "Log probability of i          is -8.400191146154116\n",
      "Log probability of look       is -11.982303986215943\n",
      "Log probability of forward    is -12.37560719314026\n",
      "Log probability of to         is -5.540690135458278\n",
      "Log probability of hearing    is -13.505537497185394\n",
      "Log probability of your       is -10.953683308276176\n",
      "Log probability of reply      is -17.623534706583847\n",
      "Log probability of .          is -4.811868102497583\n",
      "Log probability of </s>       is -4.62532894422938\n",
      " \n",
      "Log probability of Unigram model:  -89.81874501974099\n",
      "***********************************************\n",
      "Bigram Model:\n",
      "Log probability of ('<s>', 'i')         is -5.631088893700847\n",
      "Log probability of ('i', 'look')        is -8.875420257009424\n",
      "Log probability of ('look', 'forward')  is -4.345774836841731\n",
      "Log probability of ('forward', 'to')    is -2.2644156362321546\n",
      "Log probability of ('to', 'hearing')    is -13.220348094875503\n",
      "Log probability of ('hearing', 'your')  is undefined\n",
      " \n",
      "Log probability of Bigram model:  undefined\n",
      "***********************************************\n",
      "Bigram-Add-One Model:\n",
      "Log probability of ('<s>', 'i')         is -6.1552832892341485\n",
      "Log probability of ('i', 'look')        is -11.58489988238998\n",
      "Log probability of ('look', 'forward')  is -10.482231743023195\n",
      "Log probability of ('forward', 'to')    is -8.825428620790015\n",
      "Log probability of ('to', 'hearing')    is -13.82744195061677\n",
      "Log probability of ('hearing', 'your')  is -15.276633330886703\n",
      "Log probability of ('your', 'reply')    is -15.310009165842912\n",
      "Log probability of ('reply', '.')       is -15.270076277195479\n",
      "Log probability of ('.', '</s>')        is -0.6693560071854955\n",
      " \n",
      "Log probability of Bigram-Add-One model:  -97.40136026716472\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 5:\")\n",
    "sentence =\"I look forward to hearing your reply .\"\n",
    "sentenceList = uniReplaceUNK(sentence)\n",
    "#print(sentenceList)\n",
    "bigramSentenceList= bigramSentencePair(sentence)\n",
    "\n",
    "print(\"The sentence: \",f\"<s> {sentence} </s>\")\n",
    "print('Unigram Model:')\n",
    "unigramLogProb = unigramLogProb(sentenceList)\n",
    "print(\"Log probability of Unigram model: \",unigramLogProb)\n",
    "print(\"***********************************************\")\n",
    "print(\"Bigram Model:\")\n",
    "bigramLogProb = bigramLogProb(bigramSentenceList)\n",
    "print(\"Log probability of Bigram model: \", bigramLogProb)\n",
    "print(\"***********************************************\")\n",
    "print(\"Bigram-Add-One Model:\")\n",
    "bigramAddOneLogProb = bigramAddOneLogProb(bigramSentenceList)\n",
    "print(\"Log probability of Bigram-Add-One model: \", bigramAddOneLogProb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73224a",
   "metadata": {},
   "source": [
    "### Question 1.3.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9c719c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the perplexity of input sentence in each model\n",
    "def perplexity(model,sentencelist,modelOfLogProb):\n",
    "    M = len(sentencelist)\n",
    "    l = 0.0\n",
    "    if model == \"unigram\":\n",
    "        l = 1/M* modelOfLogProb\n",
    "    elif model == \"bigram\":\n",
    "        if(modelOfLogProb == 'undefined'):\n",
    "            return 'undefined'\n",
    "        else:\n",
    "            l = 1/M * modelOfLogProb\n",
    "    elif model == \"bigramAddOne\":\n",
    "        if(modelOfLogProb == 'undefined'):\n",
    "            return 'undefined'\n",
    "        else:\n",
    "            l = 1/M* modelOfLogProb\n",
    "    res = math.pow(2,(-1)*l)\n",
    "    return res\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "514e922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6:\n",
      "The perplexity of Unigram model:  1009.8046830192555\n",
      "The perplexity of Bigram model:  undefined\n",
      "The perplexity of Bigram-Add-One model:  1810.7521375332478\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 6:\")\n",
    "sentenceList = uniReplaceUNK(sentence)\n",
    "sentenceList.remove('<s>')\n",
    "# print(sentenceList)\n",
    "print(\"The perplexity of Unigram model: \",perplexity(\"unigram\",sentenceList,unigramLogProb))\n",
    "print(\"The perplexity of Bigram model: \",perplexity(\"bigram\",sentenceList,bigramLogProb))\n",
    "print(\"The perplexity of Bigram-Add-One model: \",perplexity(\"bigramAddOne\",sentenceList,bigramAddOneLogProb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893fb70e",
   "metadata": {},
   "source": [
    "### Question 1.3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e4f8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the all words list of test file\n",
    "def getWordList(inputfile):\n",
    "    file = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    wordList = []\n",
    "    for line in file:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if word != '<s>':\n",
    "                 wordList.append(word)\n",
    "    return wordList\n",
    "# get the all word pair list of test file\n",
    "def getWordPair(inputfile):\n",
    "    file = open(inputfile,\"r\",encoding=\"utf8\")\n",
    "    testBigramList =[]\n",
    "    for line in file:\n",
    "        words = line.split()\n",
    "        for i in range(len(words)-1):\n",
    "            wordPair = (words[i],words[i+1])\n",
    "            testBigramList.append(wordPair)\n",
    "    return testBigramList\n",
    "\n",
    "# Calculate the Log probability of Unigram model for entire test corpus\n",
    "def unigramTestLogProb(wordList):\n",
    "    unigramDict_Train = wordsDictionary(processed_Train)\n",
    "    unigramDict_Train.pop('<s>')\n",
    "    totalTokens = sum(unigramDict_Train.values())\n",
    "    sumOfProb =0.0\n",
    "    for word in wordList:\n",
    "        eachOfProb = math.log(unigramDict_Train[word]/totalTokens,2)\n",
    "        sumOfProb += eachOfProb\n",
    "    return sumOfProb\n",
    "\n",
    "# Calculate the Log probability of Bigram model for entire test corpus\n",
    "def bigramTestLogProb(pairList):\n",
    "    bigramDict_Train = BigramDictionary(processed_Train)\n",
    "    unigramDict_Train = wordsDictionary(processed_Train)\n",
    "    resultProb =[]\n",
    "    bigramProb =0.0\n",
    "    bigramLogProb =0.0\n",
    "    for word in pairList:\n",
    "        if word not in bigramDict_Train:\n",
    "            bigramProb = 0\n",
    "        else:\n",
    "            bigramProb = bigramDict_Train[word]/unigramDict_Train[word[0]] \n",
    "        resultProb.append(bigramProb)\n",
    "    \n",
    "    for i in resultProb:\n",
    "        if  i == 0:\n",
    "            bigramLogProb = 'undefined'\n",
    "            return bigramLogProb\n",
    "        else:\n",
    "            eachOfBigramLogProb = math.log(i,2)\n",
    "            bigramLogProb += eachOfBigramLogProb\n",
    "    return bigramLogProb\n",
    "\n",
    "# Calculate the Log probability of Bigram model with Add-one smoothing for entire test corpus\n",
    "def bigramAddOneTestLogProb(pairList):   \n",
    "    bigramDict_Train = BigramDictionary(processed_Train)\n",
    "    unigramDict_Train = wordsDictionary(processed_Train)\n",
    "    \n",
    "    resultProb =[]\n",
    "    bigramProbAdd =0.0\n",
    "    bigramAddOneLogProb =0.0\n",
    "\n",
    "    for word in pairList:\n",
    "        if word not in bigramDict_Train:\n",
    "            bigramProbAdd = 1/(unigramDict_Train[word[0]]+ len(unigramDict_Train))\n",
    "        else:\n",
    "            bigramProbAdd = (bigramDict_Train[word] + 1)/(unigramDict_Train[word[0]]+ len(unigramDict_Train))\n",
    "        resultProb.append(bigramProbAdd)\n",
    "        \n",
    "    for i in resultProb:\n",
    "        if  i == 0:\n",
    "            bigramLogProb = 'undefined'\n",
    "            return bigramLogProb\n",
    "        else:\n",
    "            eachOfBigramLogProb = math.log(i,2)\n",
    "            bigramAddOneLogProb += eachOfBigramLogProb\n",
    "    return bigramAddOneLogProb\n",
    "\n",
    "# Calculate the perplexity of the entire test corpus under each of the models\n",
    "def testPerplexity(model,wordList,modelOfLogProb):\n",
    "    M = len(wordList)\n",
    "    l = 0.0\n",
    "    if model == \"unigram\":\n",
    "        l = 1/M*(modelOfLogProb)\n",
    "    elif model == \"bigram\":\n",
    "        if(modelOfLogProb == 'undefined'):\n",
    "            return 'undefined'\n",
    "        else:\n",
    "            l = 1/M* (modelOfLogProb)\n",
    "    elif model == \"bigramAddOne\":\n",
    "        if(modelOfLogProb == 'undefined'):\n",
    "            return 'undefined'\n",
    "        else:\n",
    "            l = 1/M* (modelOfLogProb)\n",
    "    res = math.pow(2,(-1)*l)\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e1b2e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 7:\n",
      "The perplexity of Unigram model in entire test corpus:  1079.2881373074517\n",
      "The perplexity of Bigram model in entire test corpus:  undefined\n",
      "The perplexity of Bigram-Add-One model in entire test corpus:  2369.400882346745\n"
     ]
    }
   ],
   "source": [
    "print(\"Question 7:\")\n",
    "wordList = getWordList(processed_Test)\n",
    "pairList = getWordPair(processed_Test)\n",
    "# print(len(wordList))\n",
    "# print(len(pairList))\n",
    "\n",
    "unigramTestLogProb = unigramTestLogProb(wordList)\n",
    "bigramTestLogProb =bigramTestLogProb(pairList)\n",
    "bigramAddOneTestLogProb = bigramAddOneTestLogProb(pairList)\n",
    "\n",
    "\n",
    "print(\"The perplexity of Unigram model in entire test corpus: \",testPerplexity(\"unigram\",wordList,unigramTestLogProb))\n",
    "print(\"The perplexity of Bigram model in entire test corpus: \",testPerplexity(\"bigram\",wordList,bigramTestLogProb ))\n",
    "print(\"The perplexity of Bigram-Add-One model in entire test corpus: \",testPerplexity(\"bigramAddOne\",wordList,bigramAddOneTestLogProb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79cecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c2b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
